<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Verity Data Science Web Page</title>
        <link rel="icon" type="image/x-icon" href="assets/icon.png" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v6.3.0/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
        <link href = "css/prism.css" rel = "stylesheet" />
    </head>
    <body>
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-light" id="mainNav">
            <div class="container px-4 px-lg-5">
                <a class="navbar-brand" href="index.html">Verity</a>
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                    Menu
                    <i class="fas fa-bars"></i>
                </button>
                <div class="collapse navbar-collapse" id="navbarResponsive">
                    <ul class="navbar-nav ms-auto py-4 py-lg-0">
                        <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="index.html">Home</a></li>
                        <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="about.html">About</a></li>
                        <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="post.html">Data Exploration</a></li>
                        <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="contact.html">Team</a></li>
                    </ul>
                </div>
            </div>
        </nav>
        <!-- Page Header-->
        <header class="masthead" style="background-image: url('assets/img/post2-bg.jpg')">
            <div class="container position-relative px-4 px-lg-5">
                <div class="row gx-4 gx-lg-5 justify-content-center">
                    <div class="col-md-10 col-lg-8 col-xl-7">
                        <div class="post-heading">
                            <h1>The goal is to turn data into information, and information into insight.</h1>
                            <h2 class="subheading">Problems look mighty small by doing data exploration</h2>
                            <span class="meta">
                                Posted by
                                <a href="#!">Verity</a>
                                on May 16, 2023
                            </span>
                        </div>
                    </div>
                </div>
            </div>
        </header>
        <!-- Post Content-->
        <article class="mb-4">
            <div class="container px-4 px-lg-5">
                <div class="row gx-4 gx-lg-5 justify-content-center">
                    <div class="col-md-10 col-lg-8 col-xl-7">
                        <h1 class = "section-heading">Introduction to Data Exploration</h1>
                        <p> Before <b>modeling</b> our data, we first preprocess the collected data then explore its corresponding values and data types. In this section, we answer several questions which may be general in nature such as how to deal with null values, data-specific questions such as what is the longest tweet, and statistical questions such as how many are categorized as false or the mean of the data collected. Executed below are the process done in order to explore the data such as preparing the data, using natural language processing, and doing time series analysis onto it. </p>
                        <p>Before getting started we first import necessary modules for processing the data as well as visualizing it as different graphs.</p>
                        
                        <pre>
                            <code class = "language-python">
                                import pandas as pd
                                import matplotlib.pyplot as plt
                                import seaborn as sns
                            </code>
                        </pre>

                        <p>We first view the data and some of its features such as what columns have null values.</p>

                        <pre>
                            <code class = "language-python">
                                df = pd.read_csv("Dataset - Group 50 - Data.csv")
                                df = df.iloc[:105]

                                print(df)
                                print(df.isnull().sum())
                            </code>
                        </pre>

                        <!--Missing Values-->
                        <div>
                        <h3><b> Handling missing values / ensuring no missing values </b></h3>
                        <p> In this section, we first try to find all the null values then deal with them using different methods such as dropping the whole column, imputing the general data, deleting the whole row. </p>
                        <p>Shown below is the several methods used in dealing with these values. As shown in the last code block, data columns that contained null values are the optional <b>Add columns, Reviewer, Review, Account Bio, Tweet translated, Views, Remarks, Location, and Reasoning</b>. For several columns that are optional and had many null values, the team decided to drop these columns by using the drop function. </p>
                        <p>This was done the same for columns we had no literal use like the <b>Account bio, Tweet translated, Views, Remarks, Location, and Reasoning</b>.</p>
                        <p>For columns with little null values and may prove to be useful in the research question being discussed by the team, we imputated values into these data column. One example of this is the Quote tweets data column. Since one of the steps the team ensured is to have clean data regarding this data, null values are actually 0 valued datacells. Thus, we decided to impute the value 0 in the null cells in this column. This is also done for the <b>Replies</b> column. </p>
                        <p>Note as well that a similar step was done for the <b>Rating</b> column wherein the imputated value is the categorical data "False".</p>

                        <pre>
                            <code class = "language-python">
                                df = pd.read_csv("Dataset - Group 50 - Data.csv")
                                df = df.iloc[:105]
                                df.drop(['Add columns here', 'Add columns here.1', 'Add columns here.2', 'Reviewer', 'Review'], axis = 1, inplace = True)
                                df.drop(['Account bio', 'Tweet Translated', 'Views', 'Remarks', 'Location', 'Reasoning'], axis = 1, inplace = True)
                                df['Quote Tweets'].fillna(0, inplace = True)
                                df['Replies'].fillna(0, inplace = True)
                                df['Rating'].fillna('FALSE', inplace = True)

                                print(df.isnull().sum())
                            </code>
                        </pre>

                        <p> Notice in the results above that when we print out the sum of null values found in the dataset, we get 0 for all the columns. This means that we have dealt with all the null values found in our data set.</p>
                        </div>

                        <!--Ensuring formatting Consistency-->
                        <div>
                            <h3><b> Ensuring formatting consistency</b></h3>
                            <p> It's also very important to ensure that formatting consistency is attained. This is important because having an inconsistent format may cause the exploration of the data to have an error or display wrong results. Since the research team ensured that the correct data was inputted in the dataset, the team only ensured that the data type is consistent with its type of data such as the case with the <b><i>Date Posted</i></b> and <b><i>Timestamp</i></b> columns since their values is of timestamp and not strings. Having a wrong datatype may cause the values to be changed accidentally or unwillingly so having a correct datatype is also important in having formatting consistency </p>
                            <p>To do this the team just converted the values of columns <b><i>Date posted</i></b> and <b><i>Timestamp</i></b> into the datetime datatype. The team also used the <b><i>.replace</i></b> method to modify some entries in the <b><i>Rating</i></b> and <b><i>Followers</i></b> column to ensure formatting consistency. Note that these errors in the formatting were found as the team were exploring the data.</p>

                            <pre>
                                <code class="language-python">
                                    df['Date posted'] = pd.to_datetime(df['Date posted'])
                                    df['Timestamp'] = pd.to_datetime(df['Timestamp'])
                                    df['Rating'] = df['Rating'].replace('FALSE. MISLEADING', 'FALSE, MISLEADING')
                                    df['Followers'] = df['Followers'].replace('28.3K', '28,300')
                                    df['Followers'] = df['Followers'].replace('20.4K', '20,300')

                                    print(df['Date posted'].dtype)
                                    print(df['Date posted'].head(5))
                                </code>
                            </pre>
                            <p>The results above shows the first 5 data of the Date posted column as well as its datatype. </p>
                        </div>

                        <!--Categorical Encoding-->
                        <div>
                            <h3><b> Categorical Encoding </b></h3>
                            <p>In this section we do categorical encoding where we convert the categorical values into an integer format so that when we feed this data to the machine learning model we're able to give better results and predictions. There are commonly two commonly used categorical encoding methods called the one hot encoding and the integer encoding. One hot encoding is usually done in categorical values without inherent value such as being high or being low while integer encoding is done for categorical values with order such as high, medium, and low.</p>
                            <p>Shown below is the process in doing this process. Notice that the team used one hot encoding for both the two categorical data <b>Rating</b> and <b>Content type</b> since the values of this data column doesn't have and inherent ordering in their values. For the content type, the data had at most 1 type of content type while in the rating there were some data that had 2 or more ratings.</p>

                            <pre>
                                <code class = "language-python">
                                    from sklearn.preprocessing import MultiLabelBinarizer
                                    df_categorical = df[['ID', 'Content type', 'Rating']].copy()
                                    one_hot = pd.get_dummies(df_categorical['Content type'], prefix = 'Content type')
                                    df_categorical = df_categorical.join(one_hot)

                                    df_categorical['Rating'] = df_categorical['Rating'].str.split(", ")
                                    one_hot = MultiLabelBinarizer()
                                    one_hot = one_hot.fit_transform(df_categorical['Rating'])

                                    df_new = pd.concat([df_categorical, pd.DataFrame(one_hot)], axis = 1)
                                    df_new.columns = ['ID', 'Content type', 'Rating', 'Content type_Emotional', 'Content type_Rational', 'Rating_FAKE',
                                    'Rating_FALSE', 'Rating_MISLEADING', 'Rating_UNPROVEN']

                                    print(df_new.head())

                                </code>
                            </pre>

                            <p>Shown above is the output for this section. Notice that the values we're changed to 0 and 1 since one hot encoding was used.</p>
                        </div>


                        <!--NLP-->
                        <div>
                            <h3><b> Natural Language Processing </b></h3>
                            <p>After the necessary preprocessing, we move on to natural language processing wherein we standardize in a way all the values in the dataset. For example, in this section we want 'Risa' and 'risa' to be the same value as they mean the same but have different formats. We also change the unnecesarry data such as emoticons by removing symbols and emojis by converting them into words. By doing this, we ensure that every piece of data can be processed for our exploration and modeling.</p>
                            <p>To start with, we import once again the necessary modules for doing these things such as the re, copy, string, and nltk modules. We also download some necessary database so that the process is done more efficiently.</p>
                            <p>The code below shows the code for doing this process. Note also that we need a working internet for us to download the databases.</p>
                            
                            <pre>
                                <code class="language-python">
                                    import re
                                    import copy
                                    import string
                                    import nltk
                                    nltk.download('stopwords')
                                    nltk.download('punkt')
                                    nltk.download('wordnet')
                                </code>
                            </pre>
                        </div>

                        <br>

                        <!--Removal of emoji and emoticon-->
                        <div>
                            <h4><b> Removal of emoji and emoticon </b></h4>
                            <p>In this section of <b>NLP</b>, we standardize the value of the string data by removing the unnecessary symbols as well as converting the emojis into a text. Note that we usually do this for the 'Account bio' column and the 'Tweets' column but since we dropped the account bio column, we don't have to do this for that data column. In addition, this is not exactly part of our main research question but this data may be useful in the data modeling section of the project so we still process this data.</p>
                            <p>To do this, we separate the handling of the two process by making functions for each of them. The first function is called <b><i>emoji_to_word</i></b> wherein its function is to return a text version of the emoji being fed to this function. To do this, we first get the database for the emojis (note that there are thousands of emojis and not everything may be in the database) so that we know what text to return for that emoji. We then use this database to change the emojis in the data into workable texts/strings.</p>
                            <p>We do the same process for handling emoticons wherein emoticons are basically just a combination of symbols instead of being graphical like emojis. We get the database for emoticons then read it and remove these from the data rather than converting it since.</p>
                            <p>The code below shows how this process is done. The output is also shown below.</p> 

                            <pre>
                                <code class="language-python">
                                    #handling emojis
                                    url_emoji = "https://drive.google.com/uc?id=1G1vIkkbqPBYPKHcQ8qy0G2zkoab2Qv4v"
                                    df_emoji = pd.read_pickle(url_emoji)
                                    df_emoji = {v: k for k, v in df_emoji.items()}

                                    def emoji_to_word(text):
                                    for emot in df_emoji:
                                        text = re.sub(r'('+emot+')', "_".join(df_emoji[emot].replace(",","").replace(":","").split()), text)
                                    return text

                                    # handling emoticons
                                    url_emote = "https://drive.google.com/uc?id=1HDpafp97gCl9xZTQWMgP2kKK_NuhENlE"
                                    df_emote = pd.read_pickle(url_emote)

                                    def emote_to_word(text):
                                        for emot in df_emote:
                                            text = re.sub(u'('+emot+')', "_".join(df_emote[emot].replace(",","").split()), text)
                                            text = text.replace(&lt;"3", "heart" ) # not included in emoticons database
                                        return text

                                    tweets = copy.deepcopy(df['Tweet'])
                                    tweets = [emoji_to_word(t) for t in tweets]
                                    tweets = [emote_to_word(t) for t in tweets]

                                    df_tweet = pd.DataFrame({'Original': df['Tweet'], 'Without Emotes': tweets})

                                    print(df_tweet.head(5))
                                </code>
                            </pre>
                        </div>

                        <br>

                        <!--Translating and Lowercasing-->
                        <div>
                            <h4><b>Translating and Lowercasing</b></h4>
                            <p>In this section of NLP, since we finished converting all the non-characters into workable texts we focus in making the format the same. As mentioned earlier, 'Risa' and 'risa' should be the same so as to make things easier we convert all the characters into a lowercase format. We also know that there are multilingual data from this column so before converting all of these into a lowercase format, we first translate all of it into english and only then that we converted all of the characters into lowercase.</p>
                            <p>The code below shows this process. Note that we imported a translator so that we can convert the multilingual text into the same language but do note that this is not 100% accurate since there are sometimes contexts in language. The team decided to translate the languages into english since most of the tweets are in english format and filipino format.</p>
                            <p>The output for the first 5 database is also shown below.</p>

                            <pre>
                                <code class="language-python">
                                    from googletrans import Translator
                                    #translating
                                    translator = Translator()
                                    tweets_eng = [t.text for t in translator.translate(tweets, src='tl', dest='en')]
                                    df_tweet['English'] = tweets_eng

                                    #lowercasing
                                    lowercase = [t.lower() for t in tweets_eng]
                                    lowercase = [t.translate(str.maketrans('', '', string.punctuation)) for t in lowercase]

                                    df_tweet['Cleaned'] = lowercase

                                    # print(df_tweet.head(5))
                                </code>
                            </pre>
                        </div>

                        <br>

                        <!--Tokenization and Stop words removal-->
                        <div>
                            <h4><b> Tokenization and Stop words removal </b></h4>
                            <p>In this section we remove the commonly used words without any real meaning like 'as', 'like', and 'and' which we call stop words. We also tokenize in this section. Tokenizing means that we get the words either by phrase, by sentence, or by word which is important because this can be used in data modeling such as the use of machine learning. Again, though this data column is not of utmost importance in our key question this may serve useful in future processes like data modeling.</p>
                            <p>To do this, we import the commonly used stopwords and we separate the data into two which are filtered tweets and tokenized tweets. Note that we only did this for the <b><i>Tweets</i></b> data column</p>
                            
                            <pre>
                                <code class="language-python">
                                    from nltk.corpus import stopwords
                                    from nltk.tokenize import word_tokenize

                                    tweets_filtered = []
                                    tweets_tokens = []
                                    for tweet in df_tweet['Cleaned']:
                                    tweet_list = tweet.split()
                                    filtered_words = [word for word in tweet_list if word.lower() not in stopwords.words('english')] 
                                    filtered_sentence = ' '.join(filtered_words)
                                    tweets_filtered.append(filtered_sentence)
                                    token = word_tokenize(filtered_sentence)
                                    tweets_tokens.append(token)

                                    df_tweet['Filtered'] = tweets_filtered
                                    df_tweet['Tokenized'] = tweets_tokens

                                    # print(df_tweet.head(5))
                                </code>
                            </pre>
                            <p>Shown above are the first 5 rows of the database.</p>
                        </div>


                        <!--Stemming and Lemmatization-->
                        <div>
                            <p>Stemming and Lemmatization are both text normalization techniques used in preparing words for processings. Stemming refers to chopping the ends of words and removing derivational affixes. Stemming is usually used in producing variants of the word base which helpfully helps in returning the same value. For example, cleaning should also return the value for clean, cleaned, and cleans. Lemmatization on the other hand refers to the vocabulary instead of just using word reduction to analyze the word's meaning.  </p>
                            <p>To do this, we first import the stemmer and lemmatizer we are going to use which in this case is the <b>'Porter Stemmer'</b> and <b>'Wordnet Lemmatizer'.</b> We then create two arrays for both the stemmed tweets and the lemmatized tweets. Afterwards, we do the process of stemming and lemmatizing and append these to the array we initialized. We then create another column for these lemmatized and stemmed tweets.</p>
                            
                            <pre>
                                <code class="language-python">
                                    from nltk.stem import PorterStemmer, WordNetLemmatizer

                                    # Initialize the stemmer and lemmatizer
                                    stemmer = PorterStemmer()
                                    lemmatizer = WordNetLemmatizer()

                                    tweets_stem, tweets_lem = [], []

                                    def stem_lem(text):
                                    words = text.split()

                                    # Stem each word
                                    stemmed_words = [stemmer.stem(word) for word in words]
                                    
                                    # Lemmatize each word
                                    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]
                                    
                                    # Return the stemmed and lemmatized words as a tuple
                                    tweets_stem.append(stemmed_words)
                                    tweets_lem.append(lemmatized_words)

                                    return (stemmed_words, lemmatized_words)


                                    # Process each text in the array
                                    processed_texts = [stem_lem(t) for t in tweets_filtered]
                                    df_tweet['Stemmed'] = tweets_stem
                                    df_tweet['Lemmatized'] = tweets_lem

                                    # print(df_tweet.head(5))
                                </code>
                            </pre>
                            <p>Shown above are the first 5 rows of the database showing these changes</p>
                        </div>

                        <!--Time Series Analysis-->
                        <div>
                            <h3><b>Interpolation</b></h3>
                            <p>In this section, we do the interpolation part of the <b>Time Series Analysis</b>. Interpolation is a type of estimation in which known values or obtained data are used to know the value of unknown datas and insert this interpolated value into these null data. Here we do 4 different interpolation wherein we found the nearest interpolation, linear interpolation, weighted time interpolation, and probably the most known of all the average per monthly since this is also applicable in our datacolumn the <b>'Date Posted'</b> data column. The code for doing these are shown below and we used the function '.interpolate' to have an easier time doing this. In this section, we used the average per month values but we still did the other interpolation so that we have a choice for doing the processes in the future. We used this since this is the simplest method.</p>
                            <p> Next we move onto the plotting part wherein we visualize the data given using our obtained data and interpolated data.</p>

                            <pre>
                                <code class="language-python">
                                    from calendar import month_name as mn
                                    import matplotlib.dates as mdates
                                    import numpy as np

                                    grouped_df = df.groupby('Date posted').size().reset_index(name='count')
                                    grouped_df.set_index(['Date posted'], inplace = True)
                                    count_monthly = grouped_df[['count']].resample('M').sum()


                                    count_monthly['count'] = count_monthly['count'].replace(0, np.NaN)
                                    #nearest interpolation
                                    count_monthly['count_nearest'] = count_monthly['count'].interpolate(method='nearest')
                                    #linear interpolation
                                    count_monthly['count_linear'] = count_monthly['count'].interpolate(method='linear')
                                    #weighted time interpolation
                                    count_monthly['count_time'] = count_monthly['count'].interpolate(method='time')
                                    #fill missing dates using average tweets per month
                                    count_monthly['count_average'] = count_monthly['count'].replace(np.NaN, count_monthly['count'].mean())
                                    count_monthly['count'] = count_monthly['count'].replace(np.NaN, 0)
                                    print(count_monthly.head(5))
                                </code>
                            </pre>
                        </div>
                        <br>


                        <!--Data Visualization and Plotting-->
                        <div>
                            <h2><b>Data Visualization and Plotting</b></h2>
                            <p>The next step after preprocessing all the data is to visualize its contents using charts, plots, and other visualizing techniques to present the complex data and represent its relationship, trends, and pattern.</p>
                            <p>Some of the visualization techniques we used are histogram to show the distribution, heatmaps to show correlations, bar graphs to show comparison, and line graphs for comparison.</p>
                        </div>

                        <!--Plotting-->
                        <div>
                            <h3><b> Plotting </b></h3>
                            <p>In this section we discuss and show the different plots done by the team to showcase the different attributes, trends, and relationship of the acquired data. This helps us to better understand the data we gathered and explain its characteristics and help us in modeling it.</p>
                            
                            <!--Line Plot-->
                            <h3><b> Line Plot </b></h3>
                            <p>In this plot we try to answer the trends and patterns on the total counts of tweets containing misinformation/disinformation on a monthly basis among the given year range of 2016 to 2022. This type of plot helps us to understand if tweets increases or decreases significantly in a certain month. Simply put this type of plot shows the change over time of the total counts of tweets.</p>
                            <p> Shown below is the line plot with obtained value and interpolated value wherein the interpolated value is the average counts of tweets per month. Notice that the x axis are the months with their corresponding year and the y-axis is the total counts of tweets.</p>

                            <pre>
                                <code class = "language-python">
                                    years = mdates.YearLocator()
                                    months = mdates.MonthLocator()
                                    years_fmt = mdates.DateFormatter('%Y-%m')

                                    fig, axes = plt.subplots(figsize=(18,10))
                                    sns.lineplot(data=count_monthly['count_average'], label = 'count_monthly', marker = 'o')
                                    axes.xaxis.set_major_locator(months)
                                    axes.xaxis.set_major_formatter(years_fmt)
                                    axes.xaxis.set_minor_locator(months)

                                    plt.xticks(rotation = 'vertical')
                                    plt.show()
                                </code>
                            </pre>
                            
                        </div>

                        <!--Histogram-->
                        <div>
                            <h3><b> Histogram </b></h3>
                            <p> The team used <b><i>seaborn</i></b> to plot the histogram showing the count of tweets during a specific time period. In this case, the time periods are separated yearly.</p>
                            <p>For this section, we discuss the heatmap plot and its use in helping us understand the data. Heatmaps are commonly used for featuring correlation between two or more variables. In this plot we used the variables <b>Quote Tweets, Replies, Likes,</b> and <b>Followers</b>. The diagonal shows a correlation of 1 since these features the same variable. This is very similar to a matrix but in a more graphical manner. The different color shows how much the two variables being compared are correlated with each other. Note that this isn't necessarily part of answering the team's main question but may prove to be useful for future reference.</p>
                            <p>Shown below is the code for doing this heatmap as well as the heatmap being discussed. Note that for a black color this means that there is no correlation between the two variables and for a peach color shows a high correlation between them.</p>



                            <pre>
                                <code class = "language-python">
                                    sns.histplot(df['Date posted'].dropna(), kde=True, color ='red')
                                </code>
                            </pre>
                        </div>

                        <!--Heatmap-->
                        <div>
                            <h3><b> Heatmap </b></h3>
                            <p> The team used <b><i>seaborn</i></b> to plot a heatmap to visualize if there is any correlation between the number of followers of an account and the engagement on their mis/disinfo tweet.</p>
                            <p> The team used the following columns in our dataframe: <b><i>Followers, Likes, Replies, Quote Tweets</i></b>. Note that there are positive correlations between the follower count of an account to the likes, replies, and quote tweet; with the <b><i>Likes</i></b> column having the strongest correlation</p>

                            <pre>
                                <code class = "language-python">
                                    df_heatmap = df[['Followers', 'Likes', 'Replies', 'Quote Tweets']].copy()

                                    for i, row in df_heatmap.iterrows():
                                        df_heatmap.at[i, 'Followers'] = row.Followers.replace(',','')
                                    df_heatmap['Followers'] = df_heatmap['Followers'].astype('int')

                                    sns.heatmap(df_heatmap.corr())
                                </code>
                            </pre>
                        </div>

                        <!--Barplot-->
                        <div>
                            <h3><b> Barplot </b></h3>
                            <p>This type of graph shows comparison among two or more variables and can be easily understood as it shows how different or similar the obtained data are. In this section we compared the number of followers in the different account types. Note however that we did not include the media account type since the team was only able to obtain 1 (one) media type of account that had a misinformation/disinformation tweet and the team thought that this may be severely inaccurate. This led to the team comparing the number of followers on the two data types only specifically: <b>Anonymous, and Identified</b>. The graph shows that Identified twitter accounts usually have more followers than Anonymous account types.</p>
                            <p>To do this particular graph, the team used the groupby function as well as the seaborn module. The code is shown below along with the discussed bar graph.</p>


                            <pre>
                                <code class = "language-python">
                                    df_bar = df[['Followers', 'Account type']].copy()
                                    for i, row in df_bar.iterrows():
                                        df_bar.at[i, 'Followers'] = row.Followers.replace(',','')
                                    df_bar['Followers'] = df_bar['Followers'].astype('int')


                                    df_bar = df_bar.groupby('Account type', as_index=False)['Followers'].mean()

                                    sns.barplot(x =df_bar['Account type'][0:2] , y = df_bar['Followers'][0:2])
                                </code>
                            </pre>
                        </div>


                        <h2 class="section-heading">The Final Frontier</h2>
                        <p>There can be no thought of finishing for ‘aiming for the stars.’ Both figuratively and literally, it is a task to occupy the generations. And no matter how much progress one makes, there is always the thrill of just beginning.</p>
                        <p>There can be no thought of finishing for ‘aiming for the stars.’ Both figuratively and literally, it is a task to occupy the generations. And no matter how much progress one makes, there is always the thrill of just beginning.</p>
                        <blockquote class="blockquote">The dreams of yesterday are the hopes of today and the reality of tomorrow. Science has not yet mastered prophecy. We predict too much for the next year and yet far too little for the next ten.</blockquote>
                        <p>Spaceflights cannot be stopped. This is not the work of any one man or even a group of men. It is a historical process which mankind is carrying out in accordance with the natural laws of human development.</p>
                        <h2 class="section-heading">Reaching for the Stars</h2>
                        <p>As we got further and further away, it [the Earth] diminished in size. Finally it shrank to the size of a marble, the most beautiful you can imagine. That beautiful, warm, living object looked so fragile, so delicate, that if you touched it with a finger it would crumble and fall apart. Seeing this has to change a man.</p>
                        <a href="#!"><img class="img-fluid" src="assets/img/post-sample-image.jpg" alt="..." /></a>
                        <span class="caption text-muted">To go places and do things that have never been done before – that’s what living is all about.</span>
                        <p>Space, the final frontier. These are the voyages of the Starship Enterprise. Its five-year mission: to explore strange new worlds, to seek out new life and new civilizations, to boldly go where no man has gone before.</p>
                        <p>As I stand out here in the wonders of the unknown at Hadley, I sort of realize there’s a fundamental truth to our nature, Man must explore, and this is exploration at its greatest.</p>
                        <p>
                            Placeholder text by
                            <a href="http://spaceipsum.com/">Space Ipsum</a>
                            &middot; Images by
                            <a href="https://www.flickr.com/photos/nasacommons/">NASA on The Commons</a>
                        </p>
                    </div>
                </div>
            </div>
        </article>
        <!-- Footer-->
        <footer class="border-top">
            <div class="container px-4 px-lg-5">
                <div class="row gx-4 gx-lg-5 justify-content-center">
                    <div class="col-md-10 col-lg-8 col-xl-7">
                        <ul class="list-inline text-center">
                            <li class="list-inline-item">
                                <a href="#!">
                                    <span class="fa-stack fa-lg">
                                        <i class="fas fa-circle fa-stack-2x"></i>
                                        <i class="fab fa-twitter fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                            <li class="list-inline-item">
                                <a href="#!">
                                    <span class="fa-stack fa-lg">
                                        <i class="fas fa-circle fa-stack-2x"></i>
                                        <i class="fab fa-facebook-f fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                            <li class="list-inline-item">
                                <a href="#!">
                                    <span class="fa-stack fa-lg">
                                        <i class="fas fa-circle fa-stack-2x"></i>
                                        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                        </ul>
                        <div class="small text-center text-muted fst-italic">Copyright &copy; Your Website 2023</div>
                    </div>
                </div>
            </div>
        </footer>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
        <script src = "js/prism.js"></script>
    </body>
</html>
